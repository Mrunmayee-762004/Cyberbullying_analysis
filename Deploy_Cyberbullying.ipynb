{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "64149958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c61832f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "data = pd.read_csv('Cyberbullying.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61b139be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text                 cyberbullying_type_encoded\n",
       "religion             7                             7998\n",
       "age                  2                             7992\n",
       "gender               4                             7973\n",
       "ethnicity            3                             7961\n",
       "not_cyberbullying    5                             7945\n",
       "other_cyberbullying  6                             7823\n",
       "Non-Bullying         1                              638\n",
       "Bullying             0                              427\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "data['cyberbullying_type_encoded'] = labelencoder.fit_transform(data['text'])\n",
    "data[['text', 'cyberbullying_type_encoded']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa6e3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing functions\n",
    "\n",
    "# converting tweet text to lower case\n",
    "def text_lower(text):\n",
    "    return text.str.lower()\n",
    "\n",
    "# removing stopwoords from the tweet text\n",
    "def clean_stopwords(text):\n",
    "    # stopwords list that needs to be excluded from the data\n",
    "    stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from',\n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n",
    "    STOPWORDS = set(stopwordlist)\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "# cleaning and removing punctuations\n",
    "def clean_puctuations(text):\n",
    "    english_puctuations = string.punctuation\n",
    "    translator = str.maketrans('','', english_puctuations)\n",
    "    return text.translate(translator)\n",
    "# cleaning and removing repeating characters\n",
    "def clean_repeating_characters(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "\n",
    "# cleaning and removing URLs\n",
    "def clean_URLs(text):\n",
    "    return re.sub(r\"((www.[^s]+)|(http\\S+))\",\"\",text)\n",
    "\n",
    "# cleaning and removing numeric data\n",
    "def clean_numeric(text):\n",
    "    return re.sub('[0-9]+', '', text)\n",
    "\n",
    "# Tokenization of tweet text\n",
    "def tokenize_tweet(text):\n",
    "    tokenizer = RegexpTokenizer('\\w+')\n",
    "    text = text.apply(tokenizer.tokenize)\n",
    "    return text\n",
    "\n",
    "# stemming    \n",
    "def text_stemming(text):\n",
    "    st = PorterStemmer()\n",
    "    text = [st.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# lemmatization\n",
    "def text_lemmatization(text):\n",
    "    lm = WordNetLemmatizer()\n",
    "    text = [lm.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f78e789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>text</th>\n",
       "      <th>cyberbullying_type_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>word katandandr food crapilici mkr</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aussietv white mkr theblock imacelebrityau tod...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xochitlsuckkk classi whore red velvet cupcak</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jasongio meh p thank head up but not concern a...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rudhoeenglish isi account pretend kurdish acco...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48752</th>\n",
       "      <td>no not but race bait libtard jackwagon</td>\n",
       "      <td>Bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48753</th>\n",
       "      <td>wont get anyon challeng snowflak libtard ton g...</td>\n",
       "      <td>Bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48754</th>\n",
       "      <td>follow not libtardmuslim ate involv blm er</td>\n",
       "      <td>Bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48755</th>\n",
       "      <td>michaelianblack ur child ostrich w head sand p...</td>\n",
       "      <td>Bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48756</th>\n",
       "      <td>foxnew not ppl know live there love it it not ...</td>\n",
       "      <td>Bullying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48757 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text               text  \\\n",
       "0                     word katandandr food crapilici mkr  not_cyberbullying   \n",
       "1      aussietv white mkr theblock imacelebrityau tod...  not_cyberbullying   \n",
       "2           xochitlsuckkk classi whore red velvet cupcak  not_cyberbullying   \n",
       "3      jasongio meh p thank head up but not concern a...  not_cyberbullying   \n",
       "4      rudhoeenglish isi account pretend kurdish acco...  not_cyberbullying   \n",
       "...                                                  ...                ...   \n",
       "48752             no not but race bait libtard jackwagon           Bullying   \n",
       "48753  wont get anyon challeng snowflak libtard ton g...           Bullying   \n",
       "48754         follow not libtardmuslim ate involv blm er           Bullying   \n",
       "48755  michaelianblack ur child ostrich w head sand p...           Bullying   \n",
       "48756  foxnew not ppl know live there love it it not ...           Bullying   \n",
       "\n",
       "       cyberbullying_type_encoded  \n",
       "0                               5  \n",
       "1                               5  \n",
       "2                               5  \n",
       "3                               5  \n",
       "4                               5  \n",
       "...                           ...  \n",
       "48752                           0  \n",
       "48753                           0  \n",
       "48754                           0  \n",
       "48755                           0  \n",
       "48756                           0  \n",
       "\n",
       "[48757 rows x 3 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining preprocess function\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text_lower(text)\n",
    "    text = text.apply(lambda text: clean_stopwords(text))\n",
    "    text = text.apply(lambda x : clean_puctuations(x))\n",
    "    text = text.apply(lambda x: clean_repeating_characters(x))\n",
    "    text = text.apply(lambda x : clean_URLs(x))\n",
    "    text = text.apply(lambda x: clean_numeric(x))\n",
    "    text = tokenize_tweet(text)\n",
    "    text = text.apply(lambda x: text_stemming(x))\n",
    "    text = text.apply(lambda x: text_lemmatization(x))\n",
    "    text = text.apply(lambda x : \" \".join(x))\n",
    "    return text\n",
    "\n",
    "data['tweet_text'] = preprocess(data['tweet_text'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "37b183c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test\n",
    "X, y = data['tweet_text'], data['cyberbullying_type_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state= 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a80101ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=500000, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transforming the data using TF-IDF Vectorizer\n",
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features= 500000)\n",
    "vectoriser.fit(X_train)\n",
    "# print(\"No. of feature words: \",len(vectoriser.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0145b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the vectoriser\n",
    "pickle.dump(vectoriser, open('tdf_vectorizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "de27ceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectoriser.transform(X_train)\n",
    "X_test = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bded4959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8207547169811321\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "svm_model_linear = SVC(kernel= 'linear', C = 1).fit(X_train, y_train)\n",
    "svm_predictions  = svm_model_linear.predict(X_test)\n",
    "accuracy = svm_model_linear.score(X_test, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "443c5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumping the model\n",
    "pickle.dump(svm_model_linear, open('model.bin', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b0a27869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for custom input prediction\n",
    "# def custom_input_prediction(text):\n",
    "#     import nltk\n",
    "#     nltk.download('omw-1.4')\n",
    "#     text = pd.Series(text)\n",
    "#     text = preprocess(text)\n",
    "#     text = [text[0],]\n",
    "#     vectoriser = pickle.load(open(\"tdf_vectorizer\", \"rb\"))\n",
    "#     text = vectoriser.transform(text)\n",
    "#     model = pickle.load(open(\"model.bin\", \"rb\"))\n",
    "#     prediction = model.predict(text)\n",
    "#     prediction = prediction[0]\n",
    "\n",
    "#     interpretations = {\n",
    "#         0 : \"Bullying\",\n",
    "#         1 : \"Non-Bullying\",\n",
    "#         2 : \"Age\",\n",
    "#         3 : \"Ethnicity\",\n",
    "#         4 : \"Gender\",\n",
    "#         5 : \"Not Cyberbullying\",\n",
    "#         6 : \"Other Cyberbullying\",\n",
    "#         7 : \"Religion\"\n",
    "#     }\n",
    "\n",
    "#     for i in interpretations.keys():\n",
    "#         if i == prediction:\n",
    "#             return interpretations[i]\n",
    "def custom_input_prediction(text):\n",
    "    import nltk\n",
    "    nltk.download('omw-1.4')\n",
    "    text = pd.Series(text)\n",
    "    text = preprocess(text)\n",
    "    text = [text[0],]\n",
    "    # to use this function we will need to define vectoriser first \n",
    "    vectoriser = pickle.load(open(\"tdf_vectorizer\", \"rb\"))\n",
    "    text = vectoriser.transform(text)\n",
    "    model = pickle.load(open(\"model.bin\", \"rb\"))\n",
    "    prediction = model.predict(text)\n",
    "    prediction = prediction[0]\n",
    "\n",
    "    interpretations = {\n",
    "        0 : \"Bullying\",\n",
    "        1 : \"Non-Bullying\",\n",
    "        2 : \"Age\",\n",
    "        3 : \"Ethnicity\",\n",
    "        4 : \"Gender\",\n",
    "        5 : \"Not Cyberbullying\",\n",
    "        6 : \"Other Cyberbullying\",\n",
    "        7 : \"Religion\"\n",
    "    }\n",
    "\n",
    "    for i in interpretations.keys():\n",
    "        if i == prediction:\n",
    "            result = interpretations[i]\n",
    "\n",
    "            # Append the result to a CSV file\n",
    "            data = pd.DataFrame({'Prediction': [result]})\n",
    "            data.to_csv('predictions.csv', mode='a', header=False, index=False)\n",
    "            \n",
    "            return result\n",
    "\n",
    "# def custom_input_prediction(text):\n",
    "#     import nltk\n",
    "#     nltk.download('omw-1.4')\n",
    "#     text = pd.Series(text)\n",
    "#     text = preprocess(text)\n",
    "#     text = [text[0],]\n",
    "\n",
    "#     # Load the vectorizer and model\n",
    "#     vectorizer = pickle.load(open(\"tdf_vectorizer\", \"rb\"))\n",
    "#     model = pickle.load(open(\"model.bin\", \"rb\"))\n",
    "\n",
    "#     # Make a prediction\n",
    "#     text_vectorized = vectorizer.transform(text)\n",
    "#     prediction = model.predict(text_vectorized)[0]\n",
    "\n",
    "#     interpretations = {\n",
    "#         0: \"Bullying\",\n",
    "#         1: \"Non-Bullying\",\n",
    "#         2: \"Age\",\n",
    "#         3: \"Ethnicity\",\n",
    "#         4: \"Gender\",\n",
    "#         5: \"Not Cyberbullying\",\n",
    "#         6: \"Other Cyberbullying\",\n",
    "#         7: \"Religion\"\n",
    "#     }\n",
    "\n",
    "#     # Track predictions and count occurrences\n",
    "#     predictions_count = {}\n",
    "#     if \"predictions_count.pkl\" in os.listdir():  # Check if the file exists\n",
    "#         with open(\"predictions_count.pkl\", \"rb\") as file:\n",
    "#             predictions_count = pickle.load(file)\n",
    "\n",
    "#     # Update the count for the current prediction\n",
    "#     if prediction in predictions_count:\n",
    "#         predictions_count[prediction] += 1\n",
    "#     else:\n",
    "#         predictions_count[prediction] = 1\n",
    "\n",
    "#     # Save the updated predictions count\n",
    "#     with open(\"predictions_count.pkl\", \"wb\") as file:\n",
    "#         pickle.dump(predictions_count, file)\n",
    "\n",
    "#     # Find the category with maximum cyberbullying occurrences\n",
    "#     max_category = max(predictions_count, key=predictions_count.get)\n",
    "#     max_count = predictions_count[max_category]\n",
    "\n",
    "#     return interpretations[prediction], max_category, max_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3426699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mrunm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Religion\n"
     ]
    }
   ],
   "source": [
    "something = \"My Grandsons are angry about this gender free crap too! 2 in primary 2 @at high school T.he is 16 yr old ASD & got bullied as did a girl in his SEN base. He had to step in as teachers to busy on phones playing games, wee lass would have had nowhere to run if loos unisex!\"\n",
    "something_2 = \"But for u its Hinduphobia isnt it? When kashmiri pandits get killed, when a hindu girl gets raped by islamists, when radical islamic terrorism kill people in the world,u still keep quiet as if nothing is happening;but jump on when some1 says anything against islam!! #Hinduphobic\"\n",
    "new_something = \"There was certainly a more \"\"acceptable\"\" time for them to be made though in the eyes of our world at large (which also includes other jokes like rape, gaybashing, etc.) Shit, try watching Friends or Seinfeld and watch how many times they throw gay people under the bus for a laugh.\"\n",
    "print(custom_input_prediction(something_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2f87a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e8cef7217cd281fcb6a8c8db182cfa58f5552d1bd196560b3b00f853b23cb36"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
